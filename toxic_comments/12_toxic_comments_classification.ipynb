{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d17ade6",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Общий-обзор-данных\" data-toc-modified-id=\"Общий-обзор-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Общий обзор данных</a></span></li><li><span><a href=\"#Векторизация-текста\" data-toc-modified-id=\"Векторизация-текста-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Векторизация текста</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Случайный-лес\" data-toc-modified-id=\"Случайный-лес-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Случайный лес</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d4147",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe4194",
   "metadata": {},
   "source": [
    "### Общий обзор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b7ead",
   "metadata": {},
   "source": [
    "В рамках этого пункта получим общее представление о данных: ознакомимся с форматом таблицы, проверим данные на пропуски и дубликаты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93920f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\freak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\freak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\freak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pymystem3\n",
    "from pymystem3 import Mystem\n",
    "import spacy\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64173d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('C:\\\\Users\\\\freak\\\\Desktop\\\\Python\\\\ML_for_texts_project\\\\toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af850968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7079c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683d5d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7708b5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b566fc",
   "metadata": {},
   "source": [
    "Можно заметить отсутствие пропусков и явных дубликатов в данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e8d90",
   "metadata": {},
   "source": [
    "### Векторизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e202149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c61f0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def clear_text(text):\n",
    "    pattern = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    clear = pattern.split()\n",
    "    lemm = []\n",
    "    for i in range(len(clear)):\n",
    "        lemm.append(wnl.lemmatize(clear[i], get_wordnet_pos(clear[i])))\n",
    "    return \" \".join(lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92345ade",
   "metadata": {},
   "source": [
    "Добавим в таблицу столбец с очищенными и лемматизированными комментариями, чтобы использовать его для обучения моделей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab10caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['lemm_text'] = data['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abff4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not try to edit war It s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir be my hero Any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  Explanation Why the edits make under my userna...  \n",
       "1  D aww He match this background colour I m seem...  \n",
       "2  Hey man I m really not try to edit war It s ju...  \n",
       "3  More I can t make any real suggestion on impro...  \n",
       "4  You sir be my hero Any chance you remember wha...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff343cf",
   "metadata": {},
   "source": [
    "Данные подготовлены к обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81ea80",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5fbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, random_state = 123, test_size = 0.4)\n",
    "valid, test = train_test_split(test, random_state = 123, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fbfb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['lemm_text']\n",
    "X_valid = valid['lemm_text']\n",
    "X_test = test['lemm_text']\n",
    "y_train = train['toxic']\n",
    "y_valid = valid['toxic']\n",
    "y_test = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261d8114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка\n",
      "(95742,)\n",
      "(95742,)\n",
      "________\n",
      "Тестовая выборка\n",
      "(31915,)\n",
      "(31915,)\n",
      "________\n",
      "Валидационная выборка\n",
      "(31914,)\n",
      "(31914,)\n"
     ]
    }
   ],
   "source": [
    "print('Обучающая выборка')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('________')\n",
    "print('Тестовая выборка')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print('________')\n",
    "print('Валидационная выборка')\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "340a3b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf4ef9",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f25d0",
   "metadata": {},
   "source": [
    "**TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278f0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(min_df = 0.0001, stop_words=stopwords, ngram_range=(1, 3))\n",
    "tf_idf_model = count_tf_idf.fit_transform(X_train)\n",
    "train_tfidf = count_tf_idf.transform(X_train)\n",
    "test_tfidf = count_tf_idf.transform(X_test)\n",
    "valid_tfidf = count_tf_idf.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc78355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка\n",
      "(95742, 45737)\n",
      "________\n",
      "Тестовая выборка\n",
      "(31915, 45737)\n",
      "________\n",
      "Валидационная выборка\n",
      "(31914, 45737)\n"
     ]
    }
   ],
   "source": [
    "print('Обучающая выборка')\n",
    "print(train_tfidf.shape)\n",
    "print('________')\n",
    "print('Тестовая выборка')\n",
    "print(test_tfidf.shape)\n",
    "print('________')\n",
    "print('Валидационная выборка')\n",
    "print(valid_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df81b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=123,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression = LogisticRegression(fit_intercept=True, \n",
    "                                class_weight='balanced', \n",
    "                                random_state=123,\n",
    "                                solver='liblinear')\n",
    "regression_parametrs = {'C': [0.1, 1, 10], 'max_iter':[1000,2000,100]}\n",
    "\n",
    "regression_grid = GridSearchCV(regression, regression_parametrs, scoring='f1', cv=5)\n",
    "regression_grid.fit(train_tfidf, y_train)\n",
    "\n",
    "regression.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f070876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "reg_params = regression_grid.best_params_\n",
    "print(reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1707f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(fit_intercept=True, class_weight='balanced', n_jobs=-1, max_iter=reg_params['max_iter'], C=reg_params['C'], random_state=123).fit(train_tfidf, y_train)\n",
    "y_pred = model_lr.predict(valid_tfidf)\n",
    "lr_f1 = round(f1_score(y_valid, y_pred), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb2d4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.748\n"
     ]
    }
   ],
   "source": [
    "print(lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d786f2",
   "metadata": {},
   "source": [
    "**CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0fd65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(min_df = 0.0001, stop_words=stopwords, ngram_range=(1, 3))\n",
    "n_gramm_train = count_vect.fit_transform(X_train)\n",
    "n_gramm_test = count_vect.transform(X_test)\n",
    "n_gramm_valid = count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7bb359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка\n",
      "(95742, 45737)\n",
      "________\n",
      "Тестовая выборка\n",
      "(31915, 45737)\n",
      "________\n",
      "Валидационная выборка\n",
      "(31914, 45737)\n"
     ]
    }
   ],
   "source": [
    "print('Обучающая выборка')\n",
    "print(n_gramm_train.shape)\n",
    "print('________')\n",
    "print('Тестовая выборка')\n",
    "print(n_gramm_test.shape)\n",
    "print('________')\n",
    "print('Валидационная выборка')\n",
    "print(n_gramm_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "909d6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freak\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=123,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "regression = LogisticRegression(fit_intercept=True, \n",
    "                                class_weight='balanced', \n",
    "                                random_state=123,\n",
    "                                solver='liblinear'\n",
    "                               )\n",
    "regression_parametrs = {'C': [0.1, 1, 10]} # хотела подобрать max_iter, но ячейка стала выполняться больше часа\n",
    "\n",
    "regression_grid = GridSearchCV(regression, regression_parametrs, scoring='f1', cv=5)\n",
    "regression_grid.fit(n_gramm_train, y_train)\n",
    "\n",
    "regression.fit(n_gramm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cf9d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = regression_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20d1e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr2 = LogisticRegression(class_weight='balanced', n_jobs=-1, C=reg_params['C'], max_iter=1500, random_state=123).fit(n_gramm_train, y_train)\n",
    "y_pred2 = model_lr2.predict(n_gramm_valid)\n",
    "lr2_f1 = round(f1_score(y_valid, y_pred2), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b58499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744\n"
     ]
    }
   ],
   "source": [
    "print(lr2_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9deb2a",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2688a0b",
   "metadata": {},
   "source": [
    "Обучим модель случайного леса с подбором гиперпараметров GridSearchCV. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcbec17",
   "metadata": {},
   "source": [
    "**TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a4d438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              n_jobs=-1, random_state=123),\n",
       "             param_grid={'max_depth': range(4, 8, 2),\n",
       "                         'n_estimators': range(20, 40, 5)},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=123)\n",
    "forest_parametrs = { 'n_estimators': range(20, 40, 5),\n",
    "                     'max_depth': range(4, 8, 2)}\n",
    "forest_grid = GridSearchCV(forest, forest_parametrs, scoring='f1', cv=3)\n",
    "forest_grid.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5cbf2cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'n_estimators': 35}\n"
     ]
    }
   ],
   "source": [
    "forest_params = forest_grid.best_params_\n",
    "print(forest_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "463fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(random_state=123, n_jobs=-1, class_weight='balanced',\n",
    "                                     max_depth=forest_params['max_depth'],\n",
    "                                     n_estimators = forest_params['n_estimators'])\n",
    "\n",
    "forest_model.fit(train_tfidf, y_train)\n",
    "forest_model_predictions = forest_model.predict(valid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dca0048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "forest_predictions = forest_model.predict(valid_tfidf)\n",
    "forest_f1 =  round(f1_score(y_valid, forest_predictions), 3)\n",
    "print(forest_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c48deb",
   "metadata": {},
   "source": [
    "**CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2e93873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              n_jobs=-1, random_state=123),\n",
       "             param_grid={'max_depth': range(4, 8, 2),\n",
       "                         'n_estimators': range(20, 40, 5)},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest2 = RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=123)\n",
    "forest2_parametrs = { 'n_estimators': range(20, 40, 5),\n",
    "                     'max_depth': range(4, 8, 2)}\n",
    "\n",
    "forest2_grid = GridSearchCV(forest2, forest2_parametrs, scoring='f1', cv=3)\n",
    "forest2_grid.fit(n_gramm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aa8c74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'n_estimators': 35}\n"
     ]
    }
   ],
   "source": [
    "forest2_params = forest2_grid.best_params_\n",
    "print(forest2_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dff82ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest2_model = RandomForestClassifier(random_state=123, n_jobs=-1, class_weight='balanced',\n",
    "                                     max_depth=forest2_params['max_depth'],\n",
    "                                     n_estimators = forest2_params['n_estimators'])\n",
    "\n",
    "forest2_model.fit(n_gramm_train, y_train)\n",
    "forest2_model_predictions = forest2_model.predict(n_gramm_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4a57bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.322\n"
     ]
    }
   ],
   "source": [
    "forest2_predictions = forest2_model.predict(n_gramm_valid)\n",
    "forest2_f1 =  round(f1_score(y_valid, forest2_predictions), 3)\n",
    "print(forest2_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0024840",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7ead1",
   "metadata": {},
   "source": [
    "**Предсказания на тестовой выборке**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a575125",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_tfidf_preds_test = model_lr.predict(test_tfidf)\n",
    "logreg_ngramm_preds_test = model_lr2.predict(n_gramm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c43d8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest_tfidf_preds_test = forest_model.predict(test_tfidf)\n",
    "rand_forest_ngramm_preds_test = forest2_model.predict(n_gramm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e4205",
   "metadata": {},
   "source": [
    "**F1 на тестовой выборке**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02070a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_tfidf_f1_test = round(f1_score(y_test, logreg_tfidf_preds_test), 3)\n",
    "logreg_ngramm_f1_test = round(f1_score(y_test, logreg_ngramm_preds_test), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7694a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_tfidf_f1_test = round(f1_score(y_test, rand_forest_tfidf_preds_test), 3)\n",
    "forest_ngramm_f1_test = round(f1_score(y_test, rand_forest_ngramm_preds_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a88f0",
   "metadata": {},
   "source": [
    "Создадим таблицу с результатами работы моделей: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ebd55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['LogReg TF-IDF','LogReg CountVectorizer'], index = ['F1'])\n",
    "results.iloc[0] = [logreg_tfidf_f1_test, logreg_ngramm_f1_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7612f397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg TF-IDF</th>\n",
       "      <th>LogReg CountVectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.751</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LogReg TF-IDF LogReg CountVectorizer\n",
       "F1         0.751                  0.758"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9920f4",
   "metadata": {},
   "source": [
    "**Наилучший результат показала Логистическая регрессия с применением CountVectorizer**"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 46,
    "start_time": "2022-07-17T12:02:14.742Z"
   },
   {
    "duration": 135919,
    "start_time": "2022-07-18T16:25:58.337Z"
   },
   {
    "duration": 4030,
    "start_time": "2022-07-18T16:28:14.260Z"
   },
   {
    "duration": 10,
    "start_time": "2022-07-18T16:28:18.291Z"
   },
   {
    "duration": 69,
    "start_time": "2022-07-18T16:28:18.318Z"
   },
   {
    "duration": 270,
    "start_time": "2022-07-18T16:28:18.389Z"
   },
   {
    "duration": 30,
    "start_time": "2022-07-18T16:28:18.661Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "183.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
